"""PR Quality Check Bot - LLM-powered code quality analysis for GitHub PRs."""

import os
import sys
import json
import base64

from typing import Any, Dict, List, Set

from src.github_api import get_pr, list_pr_files, upsert_comment, gh_paginate
from src.openai_api import discover_context_for_rules, analyze_file_given_rules_and_context
from src.utils import env_bool, load_yaml_config, load_prompt, to_list, match_files, sanitize_patch, build_prompt
from src.rules import filter_rules_for_file


def main():
    """Main entry point for PR Quality Check Bot."""
    gh_token = os.getenv("GH_TOKEN")
    openai_key = os.getenv("OPENAI_API_KEY")
    model_ctx = os.getenv("OPENAI_MODEL_CONTEXT", "gpt-5-nano")
    model_review = os.getenv("OPENAI_MODEL_REVIEW", "gpt-5-mini")
    cfg_path = os.getenv("QUALITY_CONFIG_FILE", "quality_check.yml")
    fail_on_errors = env_bool("FAIL_ON_ERRORS", False)
    log_prompts = env_bool("LOG_PROMPTS", False)
    cache_enabled = env_bool("ANALYSIS_CACHE_ENABLED", True)

    repo_full = os.getenv("GITHUB_REPOSITORY", "")
    pr_number_env = os.getenv("PR_NUMBER", "")

    if not pr_number_env or not repo_full:
        # Not a PR event or missing context; set outputs and exit gracefully
        with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as f:
            f.write("total-errors=0\n")
            f.write("total-warnings=0\n")
            f.write("total-files-checked=0\n")
            f.write("total-changed-files=0\n")
            f.write("has-matching-changes=false\n")
            f.write("has-fix-candidates=false\n")
            f.write("fix-candidates-path=\n")
        return

    owner, repo = repo_full.split("/", 1)
    pr_number = int(pr_number_env)

    if not gh_token or not openai_key:
        raise RuntimeError("Missing GH_TOKEN or OPENAI_API_KEY")

    # Load config with friendly error handling
    try:
        cfg = load_yaml_config(cfg_path)
    except Exception as e:
        print(f"::error::Failed to parse configuration file '{cfg_path}': {e}")
        with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as f:
            f.write("total-errors=0\n")
            f.write("total-warnings=0\n")
            f.write("total-files-checked=0\n")
            f.write("total-changed-files=0\n")
            f.write("has-matching-changes=false\n")
            f.write("has-fix-candidates=false\n")
            f.write("fix-candidates-path=\n")
        return
    rules_obj = cfg.get("rules") or {}
    patterns = list(rules_obj.keys()) if isinstance(rules_obj, dict) else []
    excludes = to_list(cfg.get("exceptions"))

    pr = get_pr(gh_token, owner, repo, pr_number)
    head_sha = pr.get("head", {}).get("sha", "")
    head_sha_short = head_sha[:7] if head_sha else ""
    files_meta = list_pr_files(gh_token, owner, repo, pr_number)
    all_changed_files = [f.get("filename", "") for f in files_meta]
    all_changed_files = [f for f in all_changed_files if f]

    # Helper: read action version from package.json (printed early for visibility)
    def _get_action_version() -> str:
        try:
            action_root = os.getenv("GITHUB_ACTION_PATH") or os.path.dirname(__file__)
            package_json_path = os.path.join(action_root, "package.json")
            with open(package_json_path, "r", encoding="utf-8") as _f:
                package_data = json.loads(_f.read())
                ver = str(package_data.get("version", "")).strip()
                return ver or "unknown"
        except Exception:
            return "unknown"

    action_version_for_log = _get_action_version()
    if action_version_for_log and action_version_for_log != "unknown":
        print(f"üõ°Ô∏è PR Quality Check v{action_version_for_log}")
    else:
        print("üõ°Ô∏è PR Quality Check (version unknown)")

    matching_files = match_files(all_changed_files, patterns, excludes)
    total_changed_files = len(all_changed_files)
    total_files_checked = 0
    total_errors = 0
    total_warnings = 0
    # Run-level stats for visibility
    files_with_rules_count = 0
    total_rules_applicable = 0
    total_rules_analyzed = 0
    files_cache_hits = 0
    cached_issues_reused = 0

    if not matching_files:
        print("No files matched configured patterns ‚Äî skipping analysis.")
        print(f"Files changed in PR: {total_changed_files}")
        skip_md = [
            "# üõ°Ô∏è PR Quality Check Results",
            "",
            "‚ÑπÔ∏è **No files to analyze**",
            "",
            "No files matching the configured quality check patterns were changed in this PR.",
            "",
            "---",
            "<!-- pr-quality-check -->",
            f"*Generated by PR Quality Check* ü§ñ (based on commit {head_sha_short})",
        ]
        upsert_comment(gh_token, owner, repo, pr_number, "\n".join(skip_md))
        # Also write a short GitHub Step Summary
        step_summary = os.getenv("GITHUB_STEP_SUMMARY")
        if step_summary:
            summary_lines = [
                "# üõ°Ô∏è PR Quality Check",
                "",
                "No files to analyze (no matches to configured patterns).",
                f"- Files changed in PR: {total_changed_files}",
                f"- Files analyzed: 0",
                f"- Issues found: 0",
                f"- Warnings: 0",
                "",
            ]
            with open(step_summary, "a", encoding="utf-8") as sf:
                sf.write("\n".join(summary_lines) + "\n")
        with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as f:
            f.write("total-errors=0\n")
            f.write("total-warnings=0\n")
            f.write("total-files-checked=0\n")
            f.write(f"total-changed-files={total_changed_files}\n")
            f.write("has-matching-changes=false\n")
            f.write("has-fix-candidates=false\n")
            f.write("fix-candidates-path=\n")
        return

    analysis_prompt_template = load_prompt("analysis")
    context_prompt_template = load_prompt("context_discovery")

    # ------------------------------------------------------------
    # Simple analysis cache embedded in prior analysis comment
    # v1: per (path, sha, rule_id) -> reasons (DEPRECATED)
    # v2: per (path, sha) -> full issues (non-context rules only)
    # ------------------------------------------------------------
    def _compute_config_hash() -> str:
        try:
            qc_text = ""
            try:
                with open(cfg_path, "r", encoding="utf-8") as _f:
                    qc_text = _f.read()
            except Exception:
                qc_text = json.dumps(cfg or {}, sort_keys=True)
            action_version = _get_action_version()
            payload = "\n".join([
                qc_text,
                analysis_prompt_template or "",
                context_prompt_template or "",
                model_review or "",
                action_version or "unknown",
            ])
            import hashlib as _hash
            return _hash.sha256(payload.encode("utf-8")).hexdigest()
        except Exception:
            return ""

    CONFIG_HASH = _compute_config_hash()

    def _load_prior_cache() -> Dict[str, Any]:
        cache: Dict[str, Any] = {
            "version": 2,
            "model": model_review,
            "config_hash": CONFIG_HASH,
            "entries": [],
        }
        try:
            comments = list(gh_paginate(gh_token, f"/repos/{owner}/{repo}/issues/{pr_number}/comments"))
            # find last cache marker
            import re as _re
            marker = None
            for c in comments:
                body = (c.get("body") or "")
                for m in _re.finditer(r"pr-qc-cache:v2:b64:([A-Za-z0-9+/=]+)", body):
                    marker = m.group(1)
            if marker:
                raw = base64.b64decode(marker.encode("utf-8")).decode("utf-8")
                obj = json.loads(raw)
                if isinstance(obj, dict):
                    cache = obj
        except Exception as e:
            print(f"Warning: failed to load prior cache from comments: {e}")
        return cache

    prior_cache = _load_prior_cache() if cache_enabled else {"entries": []}
    prior_entries = prior_cache.get("entries") or []
    
    # Discard entire cache if config hash doesn't match
    if cache_enabled and prior_cache.get("config_hash") != CONFIG_HASH:
        print(f"Cache invalidated: config hash changed from {prior_cache.get('config_hash', 'none')[:8]}... to {CONFIG_HASH[:8]}...")
        prior_entries = []
        prior_cache = {"entries": []}
    
    # Build per-file map from prior cache
    file_cache_by_path: Dict[str, Dict[str, Any]] = {}
    if isinstance(prior_entries, list):
        for ent in prior_entries:
            if isinstance(ent, dict) and ent.get("path"):
                file_cache_by_path[ent["path"]] = ent
    updated_entries: List[Dict[str, Any]] = []

    # Log cache status
    try:
        print("=== Analysis cache (loaded from prior comment) ===")
        if prior_entries:
            print(f"Cache version: {prior_cache.get('version')} model: {prior_cache.get('model')} config_hash: {prior_cache.get('config_hash')}")
            print(f"Total cached entries: {len(prior_entries)}")
            # Show a readable preview
            preview = {
                "version": prior_cache.get("version"),
                "model": prior_cache.get("model"),
                "config_hash": prior_cache.get("config_hash"),
                "entries": prior_entries,
            }
            print(json.dumps(preview, ensure_ascii=False, indent=2))
        else:
            print("No prior cache found (or cache empty).")
    except Exception as _e:
        print(f"Warning: failed printing cache preview: {_e}")

    results_md: List[str] = [
        "# üõ°Ô∏è PR Quality Check Results",
        "",
        "Automated quality analysis for files in this PR that match configured patterns.",
        "",
    ]

    print(" ------------------------------------------------------------")
    print(" ------------------------------------------------------------")
    print("Starting PR Quality Check")
    print(" ------------------------------------------------------------")
    print(" ------------------------------------------------------------")
    # Collect fix candidates across all files (only for rules with auto_fix_instructions)
    fix_candidates: List[Dict[str, Any]] = []

    for fpath in matching_files:
        meta = next((m for m in files_meta if m.get("filename") == fpath), {})
        status = meta.get("status", "")
        blob_sha = meta.get("sha", "")
        prev_name = meta.get("previous_filename") or ""
        patch = meta.get("patch") or ""
        short_sha = (blob_sha or "")[:7]
        print(f"+++ File: {fpath} ({short_sha}) +++")
        if prev_name:
            eff_status = "modified" if status == "renamed" else status
            print(f"Status: {status} (treated as {eff_status})")
            print(f"Renamed from: {prev_name}")
        else:
            print(f"Status: {status or 'unknown'}")

        if status == "removed":
            print(f"Skipping deleted file: {fpath}")
            continue

        total_files_checked += 1

        rules_yaml, matched_pats, filtered_rules = filter_rules_for_file(cfg, fpath, status)
        matched_patterns_desc = ", ".join(matched_pats) if matched_pats else "none"
        print(f"Matched patterns: {matched_patterns_desc or 'none'}")

        allowed_ids: List[str] = []
        # Track which rules are allowed for this file and map by id (to access claude_code_fix)
        gated_rules: List[Dict[str, Any]] = []

        if filtered_rules:
            files_with_rules_count += 1
            all_ids = [r.get("id") for r in filtered_rules if r.get("id")]
            allowed_ids = sorted(set(all_ids))

            # Gate rules by allowed_ids
            gated_rules = [r for r in filtered_rules if r.get("id") in allowed_ids]
            try:
                total_rules_applicable += len(gated_rules)
            except Exception:
                pass

            # Log which rules remain after gating
            gated_ids = [r.get("id") for r in gated_rules if r.get("id")]
            print(f"Relevant rules: {', '.join(gated_ids) if gated_ids else 'none'}")

            # Rebuild YAML from gated rules (strip internal fields like claude_code_fix)
            if gated_rules:
                # Keep formatting simple: one flat list under a synthetic header
                import yaml as _yaml
                sanitized_rules = []
                for _r in gated_rules:
                    if not isinstance(_r, dict):
                        continue
                    sanitized = {
                    "id": _r.get("id"),
                    "description": _r.get("description")
                }
                    sanitized_rules.append(sanitized)
                rules_yaml_all = _yaml.safe_dump(sanitized_rules, sort_keys=False, allow_unicode=True).strip()
            else:
                rules_yaml_all = ""


        # Cache split (v2): per-file cache; reuse non-context issues when blob_sha matches
        cached_issues_for_file: List[Dict[str, Any]] = []
        rules_to_analyze: List[Dict[str, Any]] = []
        cached_rule_ids: List[str] = []
        cache_entry = file_cache_by_path.get(fpath) if cache_enabled else None
        cache_hit = bool(cache_entry and cache_entry.get("blob_sha") == blob_sha)
        # Pre-compute per-rule context signatures (only for rules with context_filter)
        def _compute_ctx_sig(rule: Dict[str, Any]) -> str:
            try:
                cf = rule.get("context_filter")
                pats: List[str] = []
                if isinstance(cf, str) and cf.strip():
                    pats = [cf.strip()]
                elif isinstance(cf, list):
                    pats = [str(x).strip() for x in cf if isinstance(x, str) and str(x).strip()]
                if not pats:
                    return ""
                # Build signature of candidate files (filename:sha) matching the filter
                matches: List[str] = []
                for m in files_meta:
                    fn = m.get("filename") or ""
                    if not fn:
                        continue
                    if any(match_files([fn], pats, [])[0:1]):
                        sh = (m.get("sha") or "")[:12]
                        matches.append(f"{fn}:{sh}")
                matches = sorted(set(matches))
                import hashlib as _hash
                return _hash.sha256("|".join(matches).encode("utf-8")).hexdigest() if matches else "-"
            except Exception:
                return ""
        prior_ctx_sigs = {}
        if cache_hit:
            prior_ctx_sigs = cache_entry.get("ctx_sigs") or {}
            cached_issues_for_file = list(cache_entry.get("issues") or [])
            try:
                print(f"Cache hit: file={fpath} sha={blob_sha[:7]} issues={len(cached_issues_for_file)}")
            except Exception:
                pass
            files_cache_hits += 1
            # Only analyze context-required rules whose filtered context signature changed
            rules_to_analyze = []
            for r in (gated_rules or []):
                rid = r.get("id")
                if not rid:
                    continue
                if not bool(r.get("requires_context", False)):
                    continue
                sig = _compute_ctx_sig(r)
                prev_sig = str((prior_ctx_sigs or {}).get(rid) or "")
                if sig and sig != prev_sig:
                    rules_to_analyze.append(r)
            cached_rule_ids = sorted({(it or {}).get("rule_id") for it in cached_issues_for_file if (it or {}).get("rule_id")})
        else:
            # First-time (no cache) analysis: analyze all gated rules
            rules_to_analyze = [r for r in (gated_rules or []) if r.get("id")]
            try:
                print(f"No cache or blob changed: file={fpath} sha={blob_sha[:7]} ‚Üí analyzing all gated rules")
            except Exception:
                pass

        # Per-rule decision summary with explicit reasons
        try:
            analyze_ids = [r.get("id") for r in (rules_to_analyze or []) if r.get("id")]
            print(f"Decision summary for {fpath} ({blob_sha[:7]}):")
            for r in (gated_rules or []):
                rid = r.get("id")
                if not rid:
                    continue
                is_ctx = bool(r.get("requires_context", False))
                analyzed_now = rid in (analyze_ids or [])
                if is_ctx:
                    curr_sig = _compute_ctx_sig(r)
                    prev_sig = str((prior_ctx_sigs or {}).get(rid) or "")
                    if analyzed_now:
                        if cache_hit:
                            prev_short = prev_sig[:8] if prev_sig else "-"
                            curr_short = curr_sig[:8] if curr_sig else "-"
                            reason = "context changed" if (prev_sig and curr_sig and curr_sig != prev_sig) else ("no prior context signature" if not prev_sig else "context candidates changed")
                            print(f"  - rule {rid} [context]: analyzed ({reason}: {prev_short} -> {curr_short})")
                        else:
                            print(f"  - rule {rid} [context]: analyzed (no cache for file/blob or first run)")
                    else:
                        if cache_hit:
                            sig_short = (prev_sig or curr_sig or "")[:8] if (prev_sig or curr_sig) else "-"
                            print(f"  - rule {rid} [context]: served from cache (context unchanged, sig={sig_short})")
                        else:
                            print(f"  - rule {rid} [context]: not analyzed (no cache present and not selected)")
                else:
                    if analyzed_now:
                        print(f"  - rule {rid} [no-context]: analyzed ({'file blob changed or no cache' if not cache_hit else 'forced analysis'})")
                    else:
                        if cache_hit:
                            print(f"  - rule {rid} [no-context]: served from cache (file blob unchanged)")
                        else:
                            print(f"  - rule {rid} [no-context]: not analyzed (no cache present and not selected)")
        except Exception:
            pass

        # Run context discovery only if we have context-required rules to analyze
        context_block = "No additional context files provided."
        ctx_rules_to_analyze = [r for r in rules_to_analyze if bool(r.get("requires_context", False))]
        if ctx_rules_to_analyze:
            print(f"Context discovery: running for rules: {', '.join([r.get('id') for r in ctx_rules_to_analyze if r.get('id')])}")
            print(f"Context discovery: all changed files considered: {', '.join(all_changed_files) if all_changed_files else 'none'}")
            ctx_files, per_rule_details = discover_context_for_rules(openai_key, model_ctx, fpath, all_changed_files, ctx_rules_to_analyze, log_prompts, load_prompt)
            if per_rule_details:
                print(">>> Context discovery results (per rule): ")
                for rid, details in per_rule_details.items():
                    files_list = details.get("context_files") or []
                    reason = details.get("reason") or ""
                    print(f"  - {rid}: files=[{', '.join(files_list) if files_list else 'none'}] reason={reason}")
            else:
                print(">>> Context discovery results (per rule): none")
            if not ctx_files:
                print(">>> Context discovery: no context files identified")
            else:
                blocks = []
                for cf in ctx_files:
                    mmeta = next((x for x in files_meta if x.get("filename") == cf), {})
                    cpatch = mmeta.get("patch") or ""
                    if cpatch:
                        blocks.append(f"### Context Patch: {cf}\n```diff\n{cpatch}\n```")
                if blocks:
                    context_block = "\n\n".join(blocks)
        elif any(bool(r.get("requires_context", False)) for r in (gated_rules or [])):
            print("Context discovery: skipped (all context rules served from cache)")

        # Build YAML for only rules to analyze
        rules_yaml = ""
        if rules_to_analyze:
            import yaml as _yaml
            sanitized_rules = []
            for _r in rules_to_analyze:
                sanitized_rules.append({
                    "id": _r.get("id"),
                    "description": _r.get("description")
                })
            rules_yaml = _yaml.safe_dump(sanitized_rules, sort_keys=False, allow_unicode=True).strip()

        js = {"issues": []}
        if rules_to_analyze:
            # sanitize hunk headers to remove code after @@ markers
            patch = sanitize_patch(patch)
            prompt = build_prompt(analysis_prompt_template, fpath, patch, rules_yaml, context_block)
            if log_prompts:
                print(f"=== Analysis prompt for: {fpath} ===")
                print(prompt)
            js = analyze_file_given_rules_and_context(openai_key, model_review, prompt)
            try:
                analyzed_ids = [r.get("id") for r in rules_to_analyze if r.get("id")]
                if analyzed_ids:
                    print(f"Model analyzed for {fpath} ({short_sha}): {', '.join(analyzed_ids)}")
                    total_rules_analyzed += len(analyzed_ids)
            except Exception:
                pass
        else:
            print(f"All rules cached for: {fpath} ‚Äî skipping analysis.")

        issues = js.get("issues") or []
        if not isinstance(issues, list):
            issues = []
        # Merge cached issues (both context and non-context) for rules we did NOT analyze this run
        if cached_issues_for_file:
            analyzed_ids_merge = {r.get("id") for r in (rules_to_analyze or []) if r.get("id")}
            cached_filtered = [it for it in cached_issues_for_file if (it or {}).get("rule_id") not in analyzed_ids_merge]
            if cached_filtered:
                issues = list(issues) + cached_filtered
                try:
                    cached_issues_reused += len(cached_filtered)
                except Exception:
                    pass

        # Compute counts using configured severities
        rule_by_id = {r.get("id"): r for r in (gated_rules or []) if r.get("id")}
        errors_in_file = 0
        warnings_in_file = 0
        for it in issues:
            rid = (it or {}).get("rule_id")
            if not rid or rid not in rule_by_id:
                continue
            sev = str(rule_by_id[rid].get("severity") or "warning").lower()
            if sev == "error":
                errors_in_file += 1
            else:
                warnings_in_file += 1

        total_errors += errors_in_file
        total_warnings += warnings_in_file

        # Build human-readable report line for this file (keep existing format)
        header_icon = "‚úÖ" if not issues else "‚ùå"
        line_parts: List[str] = [f"## {header_icon} {fpath}"]
        if not issues:
            line_parts.append("\nAll quality checks passed")
        else:
            msg_parts: List[str] = []
            for it in issues:
                rid = (it or {}).get("rule_id")
                reason = (it or {}).get("reason") or ""
                if not rid:
                    continue
                sev = str(rule_by_id.get(rid, {}).get("severity") or "warning").lower()
                icon = "‚ùå" if sev == "error" else "‚ö†Ô∏è"
                msg_parts.append(f"{icon} **[{rid}]**: {reason}")
            if msg_parts:
                bullets = [f"- {m}" for m in msg_parts]
                line_parts.append("\n" + "\n".join(bullets))

        results_md.append("\n".join(line_parts))
        results_md.append("")

        print(f"Outcome: {errors_in_file} errors, {warnings_in_file} warnings")

        # Build auto-fix candidates straight from JSON issues
        try:
            # Aggregate multiple occurrences per rule id, preserving unique reasons
            rule_to_agg: Dict[str, Dict[str, Any]] = {}
            for it in issues:
                rid = (it or {}).get("rule_id")
                reason = ((it or {}).get("reason") or "").strip()
                if not rid or rid not in rule_by_id:
                    continue
                fix_instr = rule_by_id[rid].get("auto_fix_instructions")
                if not fix_instr or not isinstance(fix_instr, str) or not fix_instr.strip():
                    continue
                if rid not in rule_to_agg:
                    rule_to_agg[rid] = {
                        "id": rid,
                        "fix_instruction": fix_instr.strip(),
                        "reasons": []  # list of strings
                    }
                # Append reason if non-empty and not yet present
                if reason and reason not in rule_to_agg[rid]["reasons"]:
                    rule_to_agg[rid]["reasons"].append(reason)

            aggregated_rules = list(rule_to_agg.values())
            if aggregated_rules:
                fix_candidates.append({
                    "path": fpath,
                    "rules": aggregated_rules,
                })
                print(f"Auto-fix candidates for {fpath}: {[r['id'] for r in aggregated_rules]}")
        except Exception as e:
            print(f"Warning: failed to build fix candidates for {fpath}: {e}")

        # Update per-file cache with all issues (latest blob wins) and persist context signatures
        try:
            if cache_enabled:
                all_issues_norm = [
                    {"rule_id": (it or {}).get("rule_id"), "reason": ((it or {}).get("reason") or "").strip()}
                    for it in (issues or [])
                    if (it or {}).get("rule_id")
                ]
                # Compute new ctx_sigs map for rules that declare context_filter
                ctx_sigs_new: Dict[str, str] = {}
                for r in (gated_rules or []):
                    rid = r.get("id")
                    if not rid or not bool(r.get("requires_context", False)):
                        continue
                    sig = _compute_ctx_sig(r)
                    if sig:
                        ctx_sigs_new[rid] = sig
                # Determine whether we need to write an updated cache entry
                write_cache = (not cache_hit) or bool(rules_to_analyze)
                if cache_hit and not write_cache:
                    # Write if context signatures changed vs prior
                    prior_ctx = cache_entry.get("ctx_sigs") or {}
                    if ctx_sigs_new != prior_ctx:
                        write_cache = True
                if write_cache:
                    entry_obj = {
                        "path": fpath,
                        "blob_sha": blob_sha,
                        "issues": all_issues_norm,
                    }
                    if ctx_sigs_new:
                        entry_obj["ctx_sigs"] = ctx_sigs_new
                    updated_entries.append(entry_obj)
        except Exception as e:
            print(f"Warning: failed to update cache for {fpath}: {e}")

        print(" ------------------------------------------------------------")
        print(" ------------------------------------------------------------")

    results_md.append("## üìä Summary")
    results_md.append("")
    results_md.append(f"- **Files changed in PR**: {total_changed_files}")
    results_md.append(f"- **Files analyzed**: {total_files_checked} (matched quality check patterns)")
    results_md.append(f"- **Issues found**: {total_errors}")
    results_md.append(f"- **Warnings**: {total_warnings}")
    results_md.append("")
    if (total_errors + total_warnings) == 0:
        results_md.append("‚úÖ **All checks passed!** Your code meets the quality standards.")
    else:
        if total_errors > 0:
            results_md.append("‚ùå **Please address the issues above before merging.**")
        else:
            results_md.append("‚ö†Ô∏è **Some warnings found.** Consider addressing them.")
    results_md.append("")
    results_md.append("---")
    results_md.append("<!-- pr-quality-check -->")
    # Note: We intentionally do not embed fix candidate fingerprints in this analysis comment.
    # Fingerprints are only included in auto-fix progress comments to avoid gating confusion.
    results_md.append(f"*Generated by PR Quality Check* ü§ñ - Comment updated with every commit (last refresh: {head_sha_short}) ")

    # Defer posting the comment until after we compute and append the fingerprint marker

    # Write concise summary to GitHub Step Summary for quick visibility
    step_summary = os.getenv("GITHUB_STEP_SUMMARY")
    if step_summary:
        summary_lines = [
            "# üõ°Ô∏è PR Quality Check",
            "",
            f"- Files changed in PR: {total_changed_files}",
            f"- Files analyzed: {total_files_checked}",
            f"- Issues found: {total_errors}",
            f"- Warnings: {total_warnings}",
            "",
        ]
        with open(step_summary, "a", encoding="utf-8") as sf:
            sf.write("\n".join(summary_lines) + "\n")

    # Compute fingerprint for fix candidates (counts per rule id per file; exclude reason texts)
    fingerprint = ""
    try:
        if fix_candidates:
            normalized = []
            # Ensure deterministic ordering by sorting candidate items by path first
            for item in sorted(fix_candidates, key=lambda it: it.get("path") or ""):
                path = item.get("path") or ""
                rule_entries = []
                for r in (item.get("rules") or []):
                    rid = r.get("id")
                    if not rid:
                        continue
                    reasons = r.get("reasons") or []
                    count = len(reasons) if isinstance(reasons, list) and reasons else 1
                    rule_entries.append({"id": rid, "count": int(count)})
                if path and rule_entries:
                    rule_entries = sorted(rule_entries, key=lambda x: x["id"])  # stable order
                    normalized.append({"path": path, "rules": rule_entries})
            # Sort normalized payload by path only
            normalized = sorted(normalized, key=lambda x: x["path"]) 
            import hashlib
            payload = json.dumps(normalized, sort_keys=True, separators=(",", ":"))
            fingerprint = hashlib.sha256(payload.encode("utf-8")).hexdigest()
    except Exception as e:
        print(f"Warning: failed to compute fix candidates fingerprint: {e}")

    # Log auto-fix fingerprint for debugging (not embedded in analysis comment)
    if fingerprint:
        print(f"Auto-fix candidates fingerprint: {fingerprint}")

        # Discover prior fingerprints from existing PR comments and log them
        try:
            prior_comments = list(gh_paginate(gh_token, f"/repos/{owner}/{repo}/issues/{pr_number}/comments"))
            print(f"Total PR comments fetched: {len(prior_comments)}")
            import re as _re
            prior_fps = []
            for c in prior_comments:
                body = (c.get("body") or "")
                for m in _re.finditer(r"pr-qc-fix-fingerprint:\s*([a-f0-9]{64})", body):
                    prior_fps.append(m.group(1))
            # de-duplicate, keep order
            seen_fp = set()
            unique_prior = []
            for fp in prior_fps:
                if fp in seen_fp:
                    continue
                seen_fp.add(fp)
                unique_prior.append(fp)
            if unique_prior:
                print(f"Found existing auto-fix fingerprints in comments: {', '.join(unique_prior)}")
                if fingerprint in seen_fp:
                    print("Current auto-fix fingerprint matches a previous run (no candidate changes).")
            else:
                print("Found existing auto-fix fingerprints in comments: none")
        except Exception as e:
            print(f"Warning: failed to read prior fingerprints from comments: {e}")

    # Now that the fingerprint (if any) has been computed, expose the summary body for the composite to post
    # Embed cache payload as hidden comment (v2)
    if cache_enabled:
        try:
            final_entries: List[Dict[str, Any]] = []
            updated_paths: Set[str] = {ent.get("path") for ent in (updated_entries or []) if isinstance(ent, dict) and ent.get("path")}
            for ent in (updated_entries or []):
                if isinstance(ent, dict):
                    final_entries.append(ent)
            # Since we discard cache upfront on config hash mismatch, we can simplify this
            if isinstance(prior_entries, list):
                for ent in prior_entries:
                    pth = (ent or {}).get("path")
                    if not pth or pth in updated_paths:
                        continue
                    final_entries.append(ent)
            cache_obj = {
                "version": 2,
                "model": model_review,
                "config_hash": CONFIG_HASH,
                "entries": final_entries,
            }
            cache_json = json.dumps(cache_obj, ensure_ascii=False, separators=(",", ":"))
            cache_b64 = base64.b64encode(cache_json.encode("utf-8")).decode("utf-8")
            try:
                # Show just-updated entries for clarity
                print("=== Cache write set (this run) ===")
                print(f"Entries written this run: {len(updated_entries)}")
                print(json.dumps(updated_entries, ensure_ascii=False, indent=2))
                # Show snapshot after merging prior cache (same config/model) + this run
                retained_from_previous = max(len(final_entries) - len(updated_entries), 0)
                print("=== Cache snapshot after merge ===")
                print(f"Total entries after merge: {len(final_entries)} (retained from previous: {retained_from_previous}, written this run: {len(updated_entries)})")
                print(json.dumps(cache_obj, ensure_ascii=False, indent=2))
            except Exception:
                pass
            results_md.append(f"<!-- pr-qc-cache:v2:b64:{cache_b64} -->")
        except Exception as e:
            print(f"Warning: failed to embed cache payload: {e}")

    # Print end-of-run operational summary
    try:
        print("=== PR Quality Check run summary ===")
        print(f"Files changed in PR: {total_changed_files}")
        print(f"Files matched quality rules: {files_with_rules_count}")
        print(f"Files analyzed this run: {total_files_checked}")
        print(f"Rules applicable to check/enforce: {total_rules_applicable}")
        print(f"Rules checked by model this run: {total_rules_analyzed}")
        if cache_enabled:
            print(f"Files served from cache: {files_cache_hits}")
            print(f"Issues reused from cache: {cached_issues_reused}")
        print(f"Total issues found: {total_errors + total_warnings} (errors={total_errors}, warnings={total_warnings})")
    except Exception:
        pass

    body = "\n".join(results_md)
    upsert_comment(gh_token, owner, repo, pr_number, body)
    # Compute how many issues are targeted by auto-fix (count reasons occurrences)
    targeted_issues = 0
    try:
        for item in fix_candidates:
            for r in (item.get("rules") or []):
                reasons = r.get("reasons") or []
                if isinstance(reasons, list) and reasons:
                    targeted_issues += len(reasons)
                else:
                    targeted_issues += 1
    except Exception:
        pass
    with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as f:
        f.write(f"total-errors={total_errors}\n")
        f.write(f"total-warnings={total_warnings}\n")
        f.write(f"total-files-checked={total_files_checked}\n")
        f.write(f"total-changed-files={total_changed_files}\n")
        f.write("has-matching-changes=true\n")
        f.write(f"fix-candidates-fingerprint={fingerprint}\n")
        f.write(f"total-issues-detected={total_errors + total_warnings}\n")
        f.write(f"total-issues-targeted={targeted_issues}\n")
        # Write fix candidates JSON if any and expose outputs
        try:
            if fix_candidates:
                qc_dir = ".qc"
                os.makedirs(qc_dir, exist_ok=True)
                fix_path = os.path.join(qc_dir, "fix_candidates.json")
                pr_info = {
                    "number": pr_number,
                    "headRef": pr.get("head", {}).get("ref", ""),
                    "baseRef": pr.get("base", {}).get("ref", ""),
                }
                payload = {"pr": pr_info, "files": fix_candidates}
                with open(fix_path, "w", encoding="utf-8") as jf:
                    json.dump(payload, jf, ensure_ascii=False, indent=2)
                f.write("has-fix-candidates=true\n")
                f.write(f"fix-candidates-path={fix_path}\n")
            else:
                f.write("has-fix-candidates=false\n")
                f.write("fix-candidates-path=\n")
        except Exception as e:
            print(f"Warning: failed to write fix candidates JSON: {e}")
            f.write("has-fix-candidates=false\n")
            f.write("fix-candidates-path=\n")

    if fail_on_errors and total_errors > 0:
        print(f"::error::Failing because fail-on-errors=true and TOTAL_ERRORS={total_errors}")
        sys.exit(1)


if __name__ == "__main__":
    main()
