"""PR Quality Check Bot - LLM-powered code quality analysis for GitHub PRs."""

import os
import sys
import json
import re

from typing import Any, Dict, List

from src.github_api import get_pr, list_pr_files, upsert_comment
from src.openai_api import openai_chat, discover_context_for_rules
from src.utils import env_bool, load_yaml_config, load_prompt, to_list, match_files, sanitize_patch, build_prompt
from src.rules import filter_rules_for_file


def main():
    """Main entry point for PR Quality Check Bot."""
    gh_token = os.getenv("GH_TOKEN")
    openai_key = os.getenv("OPENAI_API_KEY")
    model_ctx = os.getenv("OPENAI_MODEL_CONTEXT", "gpt-5-nano")
    model_review = os.getenv("OPENAI_MODEL_REVIEW", "gpt-5-mini")
    cfg_path = os.getenv("QUALITY_CONFIG_FILE", "quality_check.yml")
    post_comment = env_bool("POST_COMMENT", True)
    fail_on_errors = env_bool("FAIL_ON_ERRORS", False)
    log_prompts = env_bool("LOG_PROMPTS", False)

    repo_full = os.getenv("GITHUB_REPOSITORY", "")
    pr_number_env = os.getenv("PR_NUMBER", "")

    if not pr_number_env or not repo_full:
        # Not a PR event or missing context; set outputs and exit gracefully
        with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as f:
            f.write("total-errors=0\n")
            f.write("total-warnings=0\n")
            f.write("total-files-checked=0\n")
            f.write("total-changed-files=0\n")
            f.write("has-matching-changes=false\n")
        return

    owner, repo = repo_full.split("/", 1)
    pr_number = int(pr_number_env)

    if not gh_token or not openai_key:
        raise RuntimeError("Missing GH_TOKEN or OPENAI_API_KEY")

    # Load config with friendly error handling
    try:
        cfg = load_yaml_config(cfg_path)
    except Exception as e:
        print(f"::error::Failed to parse configuration file '{cfg_path}': {e}")
        with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as f:
            f.write("total-errors=0\n")
            f.write("total-warnings=0\n")
            f.write("total-files-checked=0\n")
            f.write("total-changed-files=0\n")
            f.write("has-matching-changes=false\n")
        return
    rules_obj = cfg.get("rules") or {}
    patterns = list(rules_obj.keys()) if isinstance(rules_obj, dict) else []
    excludes = to_list(cfg.get("exceptions"))

    pr = get_pr(gh_token, owner, repo, pr_number)
    head_sha = pr.get("head", {}).get("sha", "")
    head_sha_short = head_sha[:7] if head_sha else ""
    files_meta = list_pr_files(gh_token, owner, repo, pr_number)
    all_changed_files = [f.get("filename", "") for f in files_meta]
    all_changed_files = [f for f in all_changed_files if f]

    matching_files = match_files(all_changed_files, patterns, excludes)
    total_changed_files = len(all_changed_files)
    total_files_checked = 0
    total_errors = 0
    total_warnings = 0

    if not matching_files:
        if post_comment:
            skip_md = [
                "# üõ°Ô∏è PR Quality Check Results",
                "",
                "‚ÑπÔ∏è **No files to analyze**",
                "",
                "No files matching the configured quality check patterns were changed in this PR.",
                "",
                "---",
                "<!-- pr-quality-check -->",
                f"*Generated by PR Quality Check* ü§ñ (based on commit {head_sha_short})",
            ]
            upsert_comment(gh_token, owner, repo, pr_number, "\n".join(skip_md))
        # Also write a short GitHub Step Summary
        step_summary = os.getenv("GITHUB_STEP_SUMMARY")
        if step_summary:
            summary_lines = [
                "# üõ°Ô∏è PR Quality Check",
                "",
                "No files to analyze (no matches to configured patterns).",
                f"- Files changed in PR: {total_changed_files}",
                f"- Files analyzed: 0",
                f"- Issues found: 0",
                f"- Warnings: 0",
                "",
            ]
            with open(step_summary, "a", encoding="utf-8") as sf:
                sf.write("\n".join(summary_lines) + "\n")
        with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as f:
            f.write("total-errors=0\n")
            f.write("total-warnings=0\n")
            f.write("total-files-checked=0\n")
            f.write(f"total-changed-files={total_changed_files}\n")
            f.write("has-matching-changes=false\n")
        return

    analysis_prompt_template = load_prompt("analysis")

    results_md: List[str] = [
        "# üõ°Ô∏è PR Quality Check Results",
        "",
        "Automated quality analysis for files in this PR that match configured patterns.",
        "",
    ]

    print(" ------------------------------------------------------------")
    print(" ------------------------------------------------------------")
    print("Starting PR Quality Check")
    print(" ------------------------------------------------------------")
    print(" ------------------------------------------------------------")
    for fpath in matching_files:
        meta = next((m for m in files_meta if m.get("filename") == fpath), {})
        status = meta.get("status", "")
        prev_name = meta.get("previous_filename") or ""
        patch = meta.get("patch") or ""
        print(f"+++ File: {fpath} +++")
        if prev_name:
            eff_status = "modified" if status == "renamed" else status
            print(f"Status: {status} (treated as {eff_status})")
            print(f"Renamed from: {prev_name}")
        else:
            print(f"Status: {status or 'unknown'}")

        if status == "removed":
            print(f"Skipping deleted file: {fpath}")
            continue

        # sanitize hunk headers to remove code after @@ markers
        patch = sanitize_patch(patch)

        total_files_checked += 1

        rules_yaml, matched_pats, filtered_rules = filter_rules_for_file(cfg, fpath, status)
        matched_patterns_desc = ", ".join(matched_pats) if matched_pats else "none"
        print(f"Matched patterns: {matched_patterns_desc or 'none'}")

        allowed_ids: List[str] = []
        context_block = "No additional context files needed for the rules applied to this file."
        if filtered_rules:
            all_ids = [r.get("id") for r in filtered_rules if r.get("id")]
            no_ctx_ids = [r.get("id") for r in filtered_rules if r.get("id") and not bool(r.get("requires_context", False))]
            if status == "added":
                allowed_ids = sorted(set(all_ids))
            else:
                allowed_ids = sorted(set(no_ctx_ids))

            # Gate rules by allowed_ids
            gated_rules = [r for r in filtered_rules if r.get("id") in allowed_ids]

            # Log which rules remain after gating
            gated_ids = [r.get("id") for r in gated_rules if r.get("id")]
            print(f"Relevant rules: {', '.join(gated_ids) if gated_ids else 'none'}")

            # Rebuild YAML from gated rules grouped by matched patterns
            if gated_rules:
                # Keep formatting simple: one flat list under a synthetic header
                import yaml as _yaml
                rules_yaml = _yaml.safe_dump(gated_rules, sort_keys=False, allow_unicode=True).strip()

            # Only run context discovery if there are gated rules that require context
            ctx_files: List[str] = []
            requires_ctx_ids = [r.get("id") for r in gated_rules if bool(r.get("requires_context", False)) and r.get("id")]
            if not requires_ctx_ids:
                print("Context discovery: skipped (no rules require context)")
            elif status != "added":
                print(f"Context discovery: skipped for modified file; rules requiring context: {', '.join(requires_ctx_ids)}")
            elif status == "added":
                print(f"Context discovery: running for rules: {', '.join(requires_ctx_ids)}")
                print(f"Context discovery: all changed files considered: {', '.join(all_changed_files) if all_changed_files else 'none'}")
                ctx_files, per_rule_details = discover_context_for_rules(openai_key, model_ctx, fpath, all_changed_files, gated_rules, log_prompts, load_prompt)
                if per_rule_details:
                    print(">>> Context discovery results (per rule): ")
                    for rid, details in per_rule_details.items():
                        files_list = details.get("context_files") or []
                        reason = details.get("reason") or ""
                        print(f"  - {rid}: files=[{', '.join(files_list) if files_list else 'none'}] reason={reason}")
                else:
                    print(">>> Context discovery results (per rule): none")
                if not ctx_files:
                    print(">>> Context discovery: no context files identified")
            if ctx_files:
                blocks = []
                for cf in ctx_files:
                    mmeta = next((x for x in files_meta if x.get("filename") == cf), {})
                    cpatch = mmeta.get("patch") or ""
                    if cpatch:
                        blocks.append(f"### Context Patch: {cf}\n```diff\n{cpatch}\n```")
                if blocks:
                    context_block = "\n\n".join(blocks)

        prompt = build_prompt(analysis_prompt_template, fpath, patch, rules_yaml, matched_patterns_desc, context_block)

        if log_prompts:
            print(f"=== Analysis prompt for: {fpath} ===")
            print(prompt)

        resp = openai_chat(openai_key, model_review, [
            {"role": "system", "content": "You are an expert code reviewer focused on code quality standards."},
            {"role": "user", "content": prompt},
        ])

        errors_in_file = 0
        warnings_in_file = 0

        msum = re.findall(r"^SUMMARY_JSON:\s*(\{.*\})\s*$", resp, flags=re.MULTILINE)
        if msum:
            try:
                js = json.loads(msum[-1])
                if isinstance(js.get("errors"), int):
                    errors_in_file = js["errors"]
                if isinstance(js.get("warnings"), int):
                    warnings_in_file = js["warnings"]
            except Exception:
                pass
        if errors_in_file == 0 and "SUMMARY_JSON:" not in resp:
            errors_in_file = resp.count("‚ùå")
        if warnings_in_file == 0 and "SUMMARY_JSON:" not in resp:
            warnings_in_file = resp.count("‚ö†Ô∏è")

        total_errors += errors_in_file
        total_warnings += warnings_in_file

        sanitized = "\n".join([ln for ln in resp.splitlines() if not ln.startswith("SUMMARY_JSON:")])
        results_md.append(sanitized)
        results_md.append("")

        print(f"Outcome: {errors_in_file} errors, {warnings_in_file} warnings")
        print(" ------------------------------------------------------------")
        print(" ------------------------------------------------------------")

    results_md.append("## üìä Summary")
    results_md.append("")
    results_md.append(f"- **Files changed in PR**: {total_changed_files}")
    results_md.append(f"- **Files analyzed**: {total_files_checked} (matched quality check patterns)")
    results_md.append(f"- **Issues found**: {total_errors}")
    results_md.append(f"- **Warnings**: {total_warnings}")
    results_md.append("")
    if (total_errors + total_warnings) == 0:
        results_md.append("‚úÖ **All checks passed!** Your code meets the quality standards.")
    else:
        if total_errors > 0:
            results_md.append("‚ùå **Please address the issues above before merging.**")
        else:
            results_md.append("‚ö†Ô∏è **Some warnings found.** Consider addressing them.")
    results_md.append("")
    results_md.append("---")
    results_md.append("<!-- pr-quality-check -->")
    results_md.append(f"*Generated by PR Quality Check* ü§ñ - Comment updated with every commit (last refresh: {head_sha_short}) ")

    body = "\n".join(results_md)
    if post_comment:
        upsert_comment(gh_token, owner, repo, pr_number, body)

    # Write concise summary to GitHub Step Summary for quick visibility
    step_summary = os.getenv("GITHUB_STEP_SUMMARY")
    if step_summary:
        summary_lines = [
            "# üõ°Ô∏è PR Quality Check",
            "",
            f"- Files changed in PR: {total_changed_files}",
            f"- Files analyzed: {total_files_checked}",
            f"- Issues found: {total_errors}",
            f"- Warnings: {total_warnings}",
            "",
        ]
        with open(step_summary, "a", encoding="utf-8") as sf:
            sf.write("\n".join(summary_lines) + "\n")

    with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as f:
        f.write(f"total-errors={total_errors}\n")
        f.write(f"total-warnings={total_warnings}\n")
        f.write(f"total-files-checked={total_files_checked}\n")
        f.write(f"total-changed-files={total_changed_files}\n")
        f.write("has-matching-changes=true\n")

    if fail_on_errors and total_errors > 0:
        print(f"::error::Failing because fail-on-errors=true and TOTAL_ERRORS={total_errors}")
        sys.exit(1)


if __name__ == "__main__":
    main()
